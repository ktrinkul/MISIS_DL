{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "2bxS2ME4_Ozr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ajubQLxH-IJu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-0.5B-Instruct').eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-0.5B-Instruct')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# два промпта\n",
        "input_text_hedgehog = '<|im_start|>system\\nYou are a storyteller. Generate a story based on user message.<|im_end|>\\n<|im_start|>user\\nGenerate me a short story about a tiny hedgehog named Sonic.<|im_end|>\\n<|im_start|>assistant\\n'\n",
        "input_text_json = '<|im_start|>system\\nYou are a JSON machine. Generate a JSON with format {\"contractor\": string with normalized contractor name, \"sum\": decimal, \"currency\": string with uppercased 3-letter currency code} based on user message.<|im_end|>\\n<|im_start|>user\\nTransfer 100 rubles and 50 kopeck to Mike<|im_end|>\\n<|im_start|>assistant\\n'"
      ],
      "metadata": {
        "id": "RSb0Cxpj-RXi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logits = model(input_ids=..., attention_mask=...).logits\n",
        "# logits_for_first_batch_and_last_token = logits[0, -1]"
      ],
      "metadata": {
        "id": "M8SXFWUc-fv5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_file(content, filepath):\n",
        "    \"\"\"Сохраняет текст в файл\"\"\"\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        f.write(content)\n",
        "    print(f\"Файл сохранен: {filepath}\")"
      ],
      "metadata": {
        "id": "0F0ew3UiLbaa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Задача 1. Greedy Decoding"
      ],
      "metadata": {
        "id": "3PpkVFDM_wZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(input_text, max_length=1000):\n",
        "\n",
        "    # объект Encoding, в котором содержатся ids и attention_mask\n",
        "    encoding = tokenizer(input_text).encodings[0]\n",
        "\n",
        "    # токенизация encoding\n",
        "    input_ids = torch.tensor([encoding.ids]).long()\n",
        "    attention_mask = torch.tensor([encoding.attention_mask]).long()\n",
        "\n",
        "    generated_ids = input_ids.clone()\n",
        "    eos_token_id = 151645\n",
        "\n",
        "    # для отображения прогресса\n",
        "    progress_bar = tqdm(total=max_length, desc=\"Generating tokens\", unit=\"token\")\n",
        "\n",
        "    # длина генерации превысила 1000 токенов\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            # модель возвращает логиты\n",
        "            logits = model(input_ids=generated_ids, attention_mask=attention_mask).logits\n",
        "        logits_for_first_batch_and_last_token = logits[0, -1, :]\n",
        "        next_token_id = torch.argmax(logits_for_first_batch_and_last_token).item()\n",
        "\n",
        "        # сгенерировался EOS-токен с ID = 151645\n",
        "        if next_token_id == eos_token_id:\n",
        "            progress_bar.update(1)\n",
        "            progress_bar.close()\n",
        "            break\n",
        "\n",
        "        generated_ids = torch.cat([generated_ids, torch.tensor([[next_token_id]]).long()], dim=-1)\n",
        "        attention_mask = torch.cat([attention_mask, torch.tensor([[1]]).long()], dim=-1)\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    progress_bar.close()\n",
        "    return generated_ids[0].tolist()\n"
      ],
      "metadata": {
        "id": "x1foVne2_G_g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating hedgehog story...\")\n",
        "hedgehog_ids = greedy_decode(input_text_hedgehog)\n",
        "hedgehog_text = tokenizer.decode(hedgehog_ids, skip_special_tokens=True)\n",
        "print(\"\\nGenerated hedgehog story:\")\n",
        "print(hedgehog_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8ZooX9j_ctS",
        "outputId": "3389a5dd-d903-411e-8817-7c9b519e39c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating hedgehog story...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating tokens:  24%|██▎       | 236/1000 [11:56<38:39,  3.04s/token]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated hedgehog story:\n",
            "system\n",
            "You are a storyteller. Generate a story based on user message.\n",
            "user\n",
            "Generate me a short story about a tiny hedgehog named Sonic.\n",
            "assistant\n",
            "Once upon a time, in a small, cozy village nestled in the heart of the forest, there lived a tiny hedgehog named Sonic. Sonic was a curious and adventurous creature, always eager to explore the world around him. One day, while wandering through the forest, Sonic stumbled upon a hidden cave.\n",
            "\n",
            "Inside the cave, Sonic discovered a treasure chest filled with magical items. As he opened the chest, he was amazed to see that the items were not just ordinary, but enchanted. Sonic was thrilled to find that he could use the items to help others in need.\n",
            "\n",
            "From that day on, Sonic became a hero in the village. He used his magical powers to help people in need, and soon, the village was filled with people who were grateful for the help they received from Sonic.\n",
            "\n",
            "Sonic's story became a legend, and people from all over the village would tell stories about him. Sonic's adventures and his magic helped to bring joy and hope to the people of the village, and he was loved and respected by all who knew him.\n",
            "\n",
            "And so, Sonic continued to be a tiny hedgehog, always on the lookout for new adventures and helping others in need.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_to_file(hedgehog_text, 'hedgehog_text_1.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL_q9jMPLeaW",
        "outputId": "edc582f6-1ed1-42ad-e4c5-1f23edab0628"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл сохранен: hedgehog_text_1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nGenerating JSON...\")\n",
        "json_ids = greedy_decode(input_text_json)\n",
        "json_text = tokenizer.decode(json_ids, skip_special_tokens=True)\n",
        "print(\"\\nGenerated JSON:\")\n",
        "print(json_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcJnbcgV_s6X",
        "outputId": "5e41184e-ab1b-4f48-8337-c568fc49ecfd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating JSON...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating tokens:   2%|▏         | 23/1000 [00:45<32:05,  1.97s/token]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated JSON:\n",
            "system\n",
            "You are a JSON machine. Generate a JSON with format {\"contractor\": string with normalized contractor name, \"sum\": decimal, \"currency\": string with uppercased 3-letter currency code} based on user message.\n",
            "user\n",
            "Transfer 100 rubles and 50 kopeck to Mike\n",
            "assistant\n",
            "{\"contractor\": \"Mike\", \"sum\": 105, \"currency\": \"rubles\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_to_file(json_text, 'json_text.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GPj5p5ILqvQ",
        "outputId": "e2c30a5b-a163-4128-8be4-76d88abd8270"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл сохранен: json_text.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты:\n",
        "- Если запустить алгоритм несколько раз, то будут ли различаться генерации?\n",
        "\n",
        "Результат генерации не будет различаться в зависимости от параметров модели (Temperature, Top_K and Top_P не применимы для greedy decoding), так как в greedy decoding всегда выбирается максимальный по вероятности токен\n",
        "- Какие есть проблемы с таким подходом к генерации в случае с генерацией сказки и в случае с генерацией JSON?\n",
        "\n",
        "В генерации текста - простота, тк выбирается наиболее подходящий вариант\n",
        "\n",
        "В генерации JSON - может привести к ошибкам в структуре (тк они неочевидны и задаются через промт)\n",
        "- Сгенерированный текст про ёжика и сгенерированный JSON."
      ],
      "metadata": {
        "id": "k4JG-9Q5H2ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hedgehog_text"
      ],
      "metadata": {
        "id": "NyRRYgl_LJ_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 2. Sampling"
      ],
      "metadata": {
        "id": "JBD4NnF8_4JV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eWe6PUdQ_7ec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}