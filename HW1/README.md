## Эксперимент 1 
До 18 эпохи показатели (loss и ROC-AUC) на train и test улучшаются, после чего принимают одни и те же значения на всех след эпохах. Это может говорить о переобучении, так как модель выучила данные.
Оптимальное колво эпох - 18, тк после модель переобучается

Epoch 17/20, Train Loss: 0.2474, Test Loss: 0.2412, Train ROC-AUC: 0.8963, Test ROC-AUC: 0.9060
Epoch 18/20, Train Loss: 0.2471, Test Loss: 0.2410, Train ROC-AUC: 0.8964, Test ROC-AUC: 0.9061
Epoch 19/20, Train Loss: 0.2471, Test Loss: 0.2410, Train ROC-AUC: 0.8964, Test ROC-AUC: 0.9061

## Эксперимент 2
Показатели улучшились, уже на 10 эпохе результаты лучше, чем ранее было на 18
Epoch 10, Train Loss: ~0.23, Test Loss: ~0.225, Train ROC-AUC: ~0.90, Test ROC-AUC: ~0.918

## Эксперимент 3
На уже первых эпохах результаты были лучше, чем ранее:
- ROC-AUC на train и test повысилось на 0.06 и на 0.03 соответвенно
- на последних эпохах были достигнуты следующие метрики:
Epoch 10, Train Loss: ~0.229, Test Loss: ~0.217, Train ROC-AUC: ~0.913, Test ROC-AUC: ~0.921
- более плавные изменения в результатах, нет больших скачков в начале

## Эксперимент 4
p = [0.01, 0.1, 0.2, 0.5, 0.9] - вероятность временного отключения нейрона, что снижает риск переобучения
- при p = 0.2 наилучшие результаты
- при увеличении p показатели на тех же эпохах хуже, но при этом видно, что улучшение происходит более постепенно 

## Эксперимент 5
weight_decay = [0.1, 0.01, 0.001] - штраф за величину весов в функцию потерь
learning_rate = [0.01, 0.05, 0.1]
- чем больше lr, тем больше вероятность "застрять" на одних и тех же показателях, без значительных последующих ихменениях
- при маленьком lr изменения не большие, но стабильно улучшают показатели
- при wd = 0.1, значение ошибки слишком высоко, что графики получают не информативными (loss увеличивается)
- при остальных значениях результатыт близки к прошлым экспериментам